{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "D:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading data. Defining classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/initial_data.csv\", header=0)\n",
    "\n",
    "label_col = 'DriveTrain'\n",
    "classes = data[label_col].unique()\n",
    "classes_num = len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting type of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_label = data[label_col]\n",
    "categorical_features = [col for col in data.columns if data[col].dtype.name == 'object']\n",
    "categorical_features.remove(label_col)\n",
    "numerical_features = [col for col in data.columns if data[col].dtype.name != 'object']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling N/A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_processed = data.fillna(data.median(axis=0), axis=0)\n",
    "data_describe = data.describe(include=[object])\n",
    "for col in categorical_features:\n",
    "    data_processed[col] = data_processed[col].fillna(data_describe[col]['top'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalization of numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_numerical = data_processed[numerical_features]\n",
    "data_numerical = (data_numerical - data_numerical.mean()) / data_numerical.std()\n",
    "\n",
    "for col in data_numerical:\n",
    "    data_processed[col] = data_numerical[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binary_features    = [col for col in categorical_features if data_describe[col]['unique'] == 2]\n",
    "nonbinary_features = [col for col in categorical_features if data_describe[col]['unique'] > 2]\n",
    "\n",
    "\n",
    "for col in binary_features:\n",
    "    top = data_describe[col]['top']\n",
    "    top_items = data_processed[col] == top\n",
    "    data_processed.loc[top_items, col] = 0\n",
    "    data_processed.loc[np.logical_not(top_items), col] = 1    \n",
    "  \n",
    "    \n",
    "data_not_binary = pd.get_dummies(data_processed[nonbinary_features])\n",
    "\n",
    "for col in data_not_binary:\n",
    "    data_processed[col] = data_not_binary[col]\n",
    "\n",
    "data_processed = data_processed.drop(nonbinary_features, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping labell from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_processed = data_processed.drop(label_col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_list = ['Price', 'Luggage.room', 'RPM', 'Horsepower']\n",
    "data_uncorr = data_processed[features_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "data_for_pca = data_processed\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(data_for_pca)\n",
    "data_pca = pca.transform(data_for_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Size reduction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features decreased from 253 to 2 after PCA.\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features decreased from %d to %d after PCA.\" % (data_processed.shape[1], data_pca.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_processed = np.array(data_processed)\n",
    "data_uncorr = np.array(data_uncorr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data_processed\n",
    "features_num = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_sample(X, y)\n",
    "X_smote, y_smote = SMOTE().fit_sample(X, y)\n",
    "X_adasyne, y_adasyne = ADASYN().fit_sample(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(pd.get_dummies(data_label))\n",
    "y_resampled = np.array(pd.get_dummies(y_resampled))\n",
    "y_smote = np.array(pd.get_dummies(y_smote))\n",
    "y_adsyne = np.array(pd.get_dummies(y_adasyne))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_smote, y_smote, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "s = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "import keras.layers as ll\n",
    "\n",
    "model = Sequential(name=\"mlp\")\n",
    "model.add(ll.InputLayer([features_num]))\n",
    "\n",
    "#model.add(ll.Flatten())\n",
    "\n",
    "\n",
    "model.add(ll.Dense(200))\n",
    "model.add(ll.Activation('tanh'))\n",
    "\n",
    "model.add(ll.Dense(50))\n",
    "model.add(ll.Activation('tanh'))\n",
    "\n",
    "model.add(ll.Dense(classes_num, activation='softmax'))\n",
    "\n",
    "model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 253)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 200)               50800     \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 61,003\n",
      "Trainable params: 61,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "140/140 [==============================] - 0s - loss: 1.0086 - acc: 0.5071     \n",
      "Epoch 2/8\n",
      "140/140 [==============================] - 0s - loss: 0.6227 - acc: 0.8214     \n",
      "Epoch 3/8\n",
      "140/140 [==============================] - 0s - loss: 0.4241 - acc: 0.8857     \n",
      "Epoch 4/8\n",
      "140/140 [==============================] - 0s - loss: 0.3057 - acc: 0.9286     \n",
      "Epoch 5/8\n",
      "140/140 [==============================] - 0s - loss: 0.2204 - acc: 0.9571     \n",
      "Epoch 6/8\n",
      "140/140 [==============================] - 0s - loss: 0.1561 - acc: 0.9857     \n",
      "Epoch 7/8\n",
      "140/140 [==============================] - 0s - loss: 0.1108 - acc: 0.9857     \n",
      "Epoch 8/8\n",
      "140/140 [==============================] - 0s - loss: 0.0784 - acc: 1.0000     \n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/61 [==============>...............] - ETA: 0s\n",
      "Loss, Accuracy =  [0.22424462058993636, 0.93442622950819676]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoss, Accuracy = \", model.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "model_reg = Sequential(name=\"mlp\")\n",
    "model_reg.add(ll.InputLayer([features_num]))\n",
    "\n",
    "#model.add(ll.Flatten())\n",
    "\n",
    "\n",
    "model_reg.add(ll.Dense(200, kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01)))\n",
    "model_reg.add(ll.Activation('tanh'))\n",
    "\n",
    "model_reg.add(ll.Dense(50, kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01)))\n",
    "model_reg.add(ll.Activation('tanh'))\n",
    "\n",
    "model_reg.add(ll.Dense(classes_num, activation='softmax'))\n",
    "\n",
    "model_reg.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        (None, 253)               0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 200)               50800     \n",
      "_________________________________________________________________\n",
      "activation_53 (Activation)   (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 50)                10050     \n",
      "_________________________________________________________________\n",
      "activation_54 (Activation)   (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 61,003\n",
      "Trainable params: 61,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "65/65 [==============================] - 6s - loss: 24.2996 - acc: 0.3846     \n",
      "Epoch 2/8\n",
      "65/65 [==============================] - 0s - loss: 21.6151 - acc: 0.4154     \n",
      "Epoch 3/8\n",
      "65/65 [==============================] - 0s - loss: 20.0368 - acc: 0.6000     \n",
      "Epoch 4/8\n",
      "65/65 [==============================] - 0s - loss: 18.7855 - acc: 0.6615     \n",
      "Epoch 5/8\n",
      "65/65 [==============================] - 0s - loss: 17.6474 - acc: 0.6769     \n",
      "Epoch 6/8\n",
      "65/65 [==============================] - 0s - loss: 16.5495 - acc: 0.7077     \n",
      "Epoch 7/8\n",
      "65/65 [==============================] - 0s - loss: 15.5875 - acc: 0.7385     \n",
      "Epoch 8/8\n",
      "65/65 [==============================] - 0s - loss: 14.7733 - acc: 0.7846     \n"
     ]
    }
   ],
   "source": [
    "model_reg.fit(X_train, y_train, epochs=8);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28/28 [==============================] - 0s\n",
      "\n",
      "Loss, Accuracy =  [14.563441276550293, 0.6428571343421936]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoss, Accuracy = \", model_reg.evaluate(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
